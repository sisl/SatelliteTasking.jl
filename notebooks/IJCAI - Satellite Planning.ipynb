{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Setup\n",
    "import Pkg;\n",
    "# Pkg.update()\n",
    "\n",
    "# Julia Packages\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using PGFPlots\n",
    "\n",
    "# Satellite Dynamics Packages\n",
    "using SatelliteDynamics\n",
    "\n",
    "# Load SatelliteTasking - Reclone to keep version current\n",
    "Pkg.clone(\"..\") # For some reason this doens't work with Pkg.add + PackageSpec. Why?\n",
    "using SatelliteTasking\n",
    "using SatelliteTasking.SatellitePlanning\n",
    "using SatelliteTasking.Analysis\n",
    "\n",
    "# Temporary for now\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"Gurobi\")\n",
    "using JuMP\n",
    "using Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure simulation\n",
    "epc0 = Epoch(2019, 1, 1, 0, 0, 0, tsys=:UTC) # Start of time span\n",
    "epcf = Epoch(2019, 1, 2, 0, 0, 0, tsys=:UTC) # End of simulation time span\n",
    "\n",
    "# Set Simulation Time Step\n",
    "timestep = 1\n",
    "dtmax    = 5\n",
    "\n",
    "# Define Satellite Orbit\n",
    "oe   = [R_EARTH + 500e3, 0, 90.0, 0, 0, 0]\n",
    "eci0 = sOSCtoCART(oe, use_degrees=true)\n",
    "\n",
    "# Numer of perturbed orbits to simulate\n",
    "num_orbits = 3\n",
    "\n",
    "# Set Perturbation Values \n",
    "pos_error = 5000 # Position knowledge error [m]\n",
    "vel_error = 5    # Velocity knowledge error [m/s]\n",
    "orb_mean  = zeros(Float64, 6)\n",
    "orb_sdev  = vcat((pos_error/sqrt(3)*ones(Float64, 3))..., (vel_error/sqrt(3)*ones(Float64, 3))...)\n",
    "\n",
    "# Simulate true and perturbed orbits\n",
    "@time true_orbit, perturbed_orbits, eci_errors = simulate_orbits(num_orbits, epc0, epcf, eci0, orb_mean, orb_sdev, timestep=timestep, dtmax=dtmax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute True and perturbed collects\n",
    "\n",
    "# Load test images\n",
    "@time images = load_images(\"../data/landsat_test.json\", dwell_time=5.0);\n",
    "# @time images = load_images(\"../data/landsat_test_150.json\", dwell_time=5.0);\n",
    "# @time images = load_images(\"../data/landsat_test_300.json\", dwell_time=5.0);\n",
    "# @time images = load_images(\"../data/landsat_test_600.json\", dwell_time=5.0);\n",
    "num_images = length(images)\n",
    "\n",
    "# Compute true and perturbed opportunities\n",
    "@time true_opportunities, perturbed_opportunities, mean_diff, sdev_diff, missing_opportunities = compute_perturbed_opportunities(true_orbit, perturbed_orbits, images, epc_step=3600);\n",
    "\n",
    "# \n",
    "@time collects = compute_collects_by_number(true_opportunities, 10);\n",
    "\n",
    "# Compute feasible collects\n",
    "image_collects = group_image_collects(collects) # Group collects by image\n",
    "num_feasible   = 0\n",
    "for img in keys(image_collects)\n",
    "    if length(image_collects[img]) > 0\n",
    "        num_feasible += 1\n",
    "    end\n",
    "end\n",
    "pct_feasible = num_feasible/num_images*100\n",
    "\n",
    "println(\"$num_feasible out of $num_images images have collection opportunities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(sdev_diff[1, :])\n",
    "println(sdev_diff[2, :])\n",
    "println(sdev_diff[3, :])\n",
    "println(missing_opportunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Differences in Opportunities\n",
    "Axis([\n",
    "    Plots.Linear(1:24, sdev_diff[1, :], legendentry=\"Start of Window\")\n",
    "    Plots.Linear(1:24, sdev_diff[2, :], legendentry=\"End of Window\")\n",
    "    Plots.Linear(1:24, sdev_diff[3, :], legendentry=\"Window Duration\")\n",
    "    Plots.Linear(1:24, missing_opportunities, legendentry=\"Number of Missing Opportunities\")\n",
    "], width=\"10cm\", height=\"10cm\", legendPos=\"north west\", xmin=0, xmax=24, ymin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph planning\n",
    "@time path, reward, image_list = sp_graph_policy(collects, Function[constraint_agility_single_axis], horizon=0.0, allow_repeats=false)\n",
    "\n",
    "println(\"Total planning reward: $reward\")\n",
    "println(\"Number of images collected: $(length(image_list))/$num_images, $(length(image_list)/num_images*100)\")\n",
    "println(\"Number of feasible images collected: $(length(image_list))/$num_feasible, $(length(image_list)/num_feasible*100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Optimize a model with 52931 rows, 4635 columns and 110247 nonzeros\n",
      "Variable types: 0 continuous, 4635 integer (4635 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Variable types: 0 continuous, 4635 integer (4635 binary)\n",
      "\n",
      "Root relaxation: objective 1.250000e+02, 31 iterations, 0.02 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                     125.0000000 4635.00000  3608%     -    0s\n",
      "     0     0          -    0       125.00000  125.00000  0.00%     -    0s\n",
      "\n",
      "Explored 0 nodes (89 simplex iterations) in 0.16 seconds\n",
      "Thread count was 12 (of 12 available processors)\n",
      "\n",
      "Solution count 1: 125 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.250000000000e+02, best bound 1.250000000000e+02, gap 0.0000%\n",
      " 11.655609 seconds (89.29 M allocations: 2.685 GiB, 8.54% gc time)\n",
      "Total planning reward: 125.0\n",
      "Number of images collected: 125/150, 83.33333333333334\n",
      "Number of feasible images collected: 125/125, 100.0\n"
     ]
    }
   ],
   "source": [
    "# MILP planning\n",
    "@time path, reward, image_list = sp_milp_policy(collects, Function[constraint_agility_single_axis], horizon=0.0, allow_repeats=false)\n",
    "\n",
    "println(\"Total planning reward: $reward\")\n",
    "println(\"Number of images collected: $(length(image_list))/$num_images, $(length(image_list)/num_images*100)\")\n",
    "println(\"Number of feasible images collected: $(length(image_list))/$num_feasible, $(length(image_list)/num_feasible*100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sp_mdp_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MDP Planning\n",
    "\n",
    "function mdp_compute_transitions(collects::Array{Collect, 1}, constraint_list; horizon=0::Real)\n",
    "\n",
    "    # Sort Collects to ensure they are in time-asecnding order\n",
    "    sort!(collects, by = x -> x.sow)\n",
    "\n",
    "    # Compute possible transitions\n",
    "    transitions = Dict{Collect, Array{Collect, 1}}()\n",
    "\n",
    "    for start_collect in collects\n",
    "        # List of valid edges/transitions for start_collect\n",
    "        transitions[start_collect] = Array{Collect, 1}[]\n",
    "        \n",
    "        for end_collect in collects\n",
    "            # Decide to insert edge\n",
    "            if (start_collect == end_collect || \n",
    "                start_collect.image == end_collect.image ||\n",
    "                end_collect.sow < start_collect.eow)\n",
    "                # Skip insertion if same collect, image, or starts before the current\n",
    "                # collection ends (no instantaneous manevers)\n",
    "                continue\n",
    "            elseif horizon > 0 && end_collect.sow > (start_collect.eow + horizon)\n",
    "                # Since we know collects are sorted we can break building the\n",
    "                # transition grarph if the distance from the next to the next start\n",
    "                # is greater than the look-ahead horizon\n",
    "                break\n",
    "            else\n",
    "                # Set transition valid by default\n",
    "                valid_transition = true\n",
    "\n",
    "                for constraint in constraint_list\n",
    "                    # If not valid transition break early\n",
    "                    if valid_transition == false\n",
    "                        continue\n",
    "                    end\n",
    "\n",
    "                    # Use logical and to evaulate path feasibility on transitions\n",
    "                    valid_transition = valid_transition && constraint(start_collect, end_collect)\n",
    "                end\n",
    "\n",
    "                if valid_transition\n",
    "                    push!(transitions[start_collect], end_collect)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    states = sort!(collect(keys(transitions)), by = x -> x.sow)\n",
    "\n",
    "    return states, transitions\n",
    "end\n",
    "\n",
    "function _mdp_update_state_values(Uk::Dict{Collect, <:Real}, Ukp::Dict{Collect, <:Real}, states::Array{Collect, 1}, transitions::Dict{Collect, Array{Collect, 1}})\n",
    "    # Perform initial update\n",
    "    for s in states\n",
    "\n",
    "        r_s = nothing # State reward\n",
    "        if length(transitions[s]) == 0\n",
    "            r_s = 0.0\n",
    "        end \n",
    "\n",
    "        for a in transitions[s]\n",
    "            # Transition probability to action state is always 1.0\n",
    "            r_a = a.image.reward + Uk[a]\n",
    "\n",
    "            # Update optimal state value \n",
    "            if r_s == nothing || r_a > r_s\n",
    "                r_s = r_a\n",
    "            end\n",
    "\n",
    "        Ukp[s] = r_s\n",
    "\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function _mdp_extract_policy(U::Dict{Collect, <:Real}, states::Array{Collect, 1}, transitions::Dict{Collect, Array{Collect, 1}})\n",
    "    policy = Dict{Collect, Union{Collect, Nothing}}()\n",
    "\n",
    "    for s in states\n",
    "        r_s = nothing\n",
    "        if length(transitions[s]) == 0\n",
    "            r_s = 0.0\n",
    "        end\n",
    "\n",
    "        a_max = nothing\n",
    "        for a in transitions[s]\n",
    "            # Transition probability to action state is always 1.0\n",
    "            r_a = a.image.reward + U[a]\n",
    "\n",
    "            # Update optimal state value \n",
    "            if r_s == nothing || r_a > r_s\n",
    "                r_s   = r_a\n",
    "                a_max = a\n",
    "            end\n",
    "        end\n",
    "\n",
    "        policy[s] = a_max\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end\n",
    "\n",
    "function mdp_value_iteration(states::Array{Collect, 1}, transitions::Dict{Collect, Array{Collect, 1}}, eps=1e-6::Real)\n",
    "\n",
    "    # Initialize Initial value function \n",
    "    Uk  = Dict{Collect, Float64}(s => 0.0 for s in states)\n",
    "    Ukp = Dict{Collect, Float64}(s => 0.0 for s in states)\n",
    "    k   = 1\n",
    "\n",
    "    # Perofrm first round of value ierations\n",
    "    _mdp_update_state_values(Uk, Ukp, states, transitions)\n",
    "\n",
    "    # Iterate while not-converged\n",
    "    resid = norm([Ukp[s] - Uk[s] for s in states])\n",
    "    while resid > eps\n",
    "        # Update values in UkP\n",
    "        for (k,v) in Ukp\n",
    "            Uk[k] = v\n",
    "        end\n",
    "\n",
    "        _mdp_update_state_values(Uk, Ukp, states, transitions)\n",
    "        resid = norm([Ukp[s] - Uk[s] for s in states])\n",
    "        k += 1\n",
    "\n",
    "#         println(\"Finished iteration $k. Bellman residual: $resid\")\n",
    "    end\n",
    "\n",
    "    println(\"Finished value iteration. Converged after $k iterations with Bellman residual: $resid\")\n",
    "\n",
    "    # Extract optimal policy from state values\n",
    "    policy = _mdp_extract_policy(Ukp, states, transitions) \n",
    "\n",
    "    # Extract optimal path from policy\n",
    "    max_s    = findmax(Ukp)[2]\n",
    "    opt_path = Union{Collect,Nothing}[max_s]\n",
    "\n",
    "    while policy[opt_path[end]] != nothing\n",
    "        push!(opt_path, policy[opt_path[end]])\n",
    "    end\n",
    "\n",
    "    return Ukp, opt_path\n",
    "end\n",
    "\n",
    "function sp_mdp_policy(collects::Array{Collect, 1}, constraint_list; horizon=0::Real, allow_repeats=false::Bool, eps=1e-6::Real)\n",
    "    # Compute States and Transitions\n",
    "    states, transitions = mdp_compute_transitions(collects, constraint_list, horizon=horizon)\n",
    "\n",
    "    # Solve graph for taskign policy\n",
    "    Ukp, path = mdp_value_iteration(states, transitions, eps)\n",
    "\n",
    "    reward     = findmax(Ukp)[2]\n",
    "    image_list = [col.image for col in path]\n",
    "\n",
    "    return path, reward, image_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished value iteration. Converged after 419 iterations with Bellman residual: 0.0\n",
      "148.756090 seconds (43.01 M allocations: 1.591 GiB, 0.51% gc time)\n",
      "Total planning reward: Collect(Ptr: 4645994912, Orbit: 0, Image: 5019237488, Opportunity: 4611716000, Start: Epoch(2019-01-01T00:10:04.000Z), End: Epoch(2019-01-01T00:10:09.000Z))\n",
      "Number of images collected: 419/150, 279.33333333333337\n",
      "Number of feasible images collected: 419/125, 335.2\n"
     ]
    }
   ],
   "source": [
    "# MDP Planning\n",
    "\n",
    "@time path, reward, image_list = sp_mdp_policy(collects, Function[constraint_agility_single_axis], horizon=0.0, allow_repeats=false)\n",
    "\n",
    "println(\"Total planning reward: $reward\")\n",
    "println(\"Number of images collected: $(length(image_list))/$num_images, $(length(image_list)/num_images*100)\")\n",
    "println(\"Number of feasible images collected: $(length(image_list))/$num_feasible, $(length(image_list)/num_feasible*100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
